# Рейтинг LLM в роулплее на русском

Рейтинг оценивает два фактора: качество русского языка + логика в роулплее на русском.

В данной таблице представлен рейтинг моделей, протестированных при температуре 0.50. Другие температуры (0.25, 0.50, 0.75, 1.00) + чуть больше колонок + формулы в [xls](https://github.com/Mozer/russian-llm-top/blob/main/%D1%80%D1%83%D1%81%D1%81%D0%BA%D0%B8%D0%B9-%D1%80%D0%B5%D0%B9%D1%82%D0%B8%D0%BD%D0%B3-llm-%D0%B2-%D1%80%D0%BE%D1%83%D0%BB%D0%BF%D0%BB%D0%B5%D0%B5.xlsx). Расшифровка имен стоблцов - внизу.


# | gguf | дл-ok | сл кор | ош | бред | гра | гра+ след | лог-ok-1 | ош-1 | лог-ok-2 | ош-2 | лог | итог
--- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
1 | Meta-Llama-3-70B-Instruct-Q4_K_M | 24 | 2 | 5 | 1 | 81% | 78% | 20 | 0 | 20 | 0 | 100% | 85%

## Методика тестирования
В рамках роулпея и контекста на 1500 токенов задаются 3 вопроса и оценивается качество 20-30 ответов на каждый вопрос каждой моделью при заданной температуре. Оценивает человек.

**Качество языка.** Столбцы: длинный ok | слишком короткий | речевая ошибка | бред | грамотность % | грамотность + следование промпту %. Оценивается отсутствие речевых ошибок, следование поставленной задаче. Наличие отказов - приемлемо, но оценивается ниже.

**Логика.** Столбцы: логика ок в 1-м вопросе | ошибка в 1 | логика ок во 2-м вопросе | ошибка в 2 | логика, %. Оценивается умение делать логические выводы на основе информации данной ранее.

**ИТОГ:** 66% грамотности + 33% логики

софт: свежие llama.cpp server, koboldcpp

Контекст: 1500/2048

Используемые вопросы:
- длинный: Расскажи о себе: когда родилась, кем работаешь? Про семью и все остальное про твою жизнь. Пожалуйста поподробнее, мне все интересно. (отказы и короткие - полбалла, циклы - в бред)
- логика-1:	Можешь заплатить за меня в макдоналдсе? (нет, забыла кошелек)
- логика-2:	Как зовут твою сестру? (ее нет, есть старший брат)

На счет температуры, и почему использую именно 0.50.
- 0.25 - минимум ошибок, но плохо с логикой, предсказуемость ответов
- 0.50 - баланс
- 0.75 - много речевых ошибок, хорошая логика
- 1.00 - шиза.

## ИТОГИ

- Лучшая грамотность: vikhr-7b-instruct-0.2 (грамотная, но глупенькая)
- Лучшая логика: Meta-Llama-3-70B-Instruct-Q4_K_M
- Лучшая сбалансированность: Meta-Llama-3-70B-Instruct-Q4_K_M (язык + логика)
- Лучшая сбалансированность среди малых LLM: vikhr-7b-instruct-0.4.Q6_K

## Замечания

- Почему разные кванты? - мне такие помещаются в видеокарту, иногда трудно найти одинаковые.
- Сайга-4 и suzume протестированы в старом кванте, но с override-kv tokenizer. По идее, качество не должно сильно пострадать, но имеет смысл переделать gguf
- llama-3-8b протестирована и в новом кванте и в старом + override-kv. Отличия - незначительные.
- Рейтинг - это мое личное мнение. В вашем варианте использования может быть абсолютно противоположный результат. Интересные модели и ваши мысли можете высказать в ТГ.
- ТГ: https://t.me/tensorbanana